{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with PyTorch \n",
    "\n",
    "-> https://pythonprogramming.net/introduction-deep-learning-neural-network-pytorch/ \n",
    "\n",
    "What's a tensor?!\n",
    "\n",
    "You can just think of a tensor like an array. Really all we're doing is basically multiplying arrays here. That's all there is to it. The fancy bits are when we run an optimization algorithm on all those weights to start modifying them. Neural networks themselves are actually super basic and simple. Their optimization is a little more challenging, but most of these deep learning libraries also help you a bit with that math. If you want to learn how to do everything yourself by hand, stay tuned later in the series. I just don't think it would be wise to lead with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  3.])\n"
     ]
    }
   ],
   "source": [
    "# standard imports \n",
    "\n",
    "import torch \n",
    "\n",
    "# tensors are like arrays so...\n",
    "\n",
    "x = torch.Tensor([5,3]) # establishes a 1 by 2 array with values 5 and 3 \n",
    "y = torch.Tensor([2,1]) # establises a 1 by 2 array with values 2 and 1 \n",
    "\n",
    "print(x*y) # multiplies the corresponding values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Array Zeros: tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "Rows, Columns Size: torch.Size([2, 5])\n",
      "Random Tensor: tensor([[0.0025, 0.5771, 0.9220, 0.4199, 0.7805],\n",
      "        [0.8364, 0.4441, 0.3622, 0.1998, 0.4067]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([2,5]) # creates a two (row) by 5 (column) array filled with zeros. \n",
    "x_shape = x.shape # outputs the row by column \n",
    "y = torch.rand([2,5]) # creates a two by 5 array filled with random digits between 0 and 1. \n",
    "\n",
    "print(\"Printing Array Zeros: \" + str(x))\n",
    "print(\"Rows, Columns Size: \" + str(x_shape))\n",
    "print(\"Random Tensor: \" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0025, 0.5771, 0.9220, 0.4199, 0.7805, 0.8364, 0.4441, 0.3622, 0.1998,\n",
       "         0.4067]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visible_y = y.view([1,10]) # for the first ten values \n",
    "visible_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "\n",
    "import torch \n",
    "import torchvision # collection to benchmark with vision tasks \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a dataset. Next, we need to handle for how we're going to iterate over that dataset. \n",
    "\n",
    "Training and Testing data split\n",
    "To train any machine learning model, we want to first off have training and validation datasets. This is so we can use data that the machine has never seen before to \"test\" the machine.\n",
    "\n",
    "Shuffling\n",
    "Then, within our training dataset, we generally want to randomly shuffle the input data as much as possible to hopefully not have any patterns in the data that might throw the machine off.\n",
    "\n",
    "For example, if you fed the machine a bunch of images of zeros, the machine would learn to classify everything as zero. Then you'd start feeding it ones, and the machine would figure out pretty quick to classify everything as ones...and so on. Whenever you stop, the machine would probably just classify everything as the last thing you trained on. If you shuffle the data, your machine is much more likely to figure out what's what.\n",
    "\n",
    "Scaling and normalization\n",
    "Another consideration at some point in the pipeline is usually scaling/normalization of the dataset. In general, we want all input data to be between zero and one. Often many datasets will contain data in ranges that are not within this range, and we generally will want to come up with a way to scale the data to be within this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set implementation from library \n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# kernel crashes \n",
    "\n",
    "train = datasets.MNIST('', train=True, download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "            ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "            ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([1, 2, 4, 6, 9, 2, 6, 9, 4, 9])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset: \n",
    "    print(data) \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "x,y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing data set \n",
    "total = 0 \n",
    "counter_dict = {0:0, 1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By \"balance,\" I mean make sure there are the same number of examples for each classifications in training.\n",
    "\n",
    "Sometimes, this simply isn't possible. There are ways for us to handle for this with special class weighting for the optimizer to take note of, but, even this doesn't always work. Personally, I've never had success with this in any real world application.\n",
    "\n",
    "In our case, how might we confirm the balance of data? Well, we just need to iterate over everything and make a count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "0: 9.871666666666666%\n",
      "1: 11.236666666666666%\n",
      "2: 9.93%\n",
      "3: 10.218333333333334%\n",
      "4: 9.736666666666666%\n",
      "5: 9.035%\n",
      "6: 9.863333333333333%\n",
      "7: 10.441666666666666%\n",
      "8: 9.751666666666667%\n",
      "9: 9.915000000000001%\n"
     ]
    }
   ],
   "source": [
    "for data in trainset: \n",
    "    Xs, ys = data \n",
    "    for y in ys: \n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1 \n",
    "\n",
    "print(counter_dict)\n",
    "\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100.0}%\")\n",
    "\n",
    "# what this does is to print out the percentages of each number within the training data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will be making our neural network model.\n",
    "\n",
    "Previously, we were just playing and importing the datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch.nn import gives us access to some helpful neural network things, such as various neural network layer types (things like regular fully-connected layers, convolutional layers (for imagery), recurrent layers...etc). For now, we've only spoken about fully-connected layers, so we will just be using those for now.\n",
    "\n",
    "The torch.nn.functional area specifically gives us access to some handy functions that we might not want to write ourselves. We will be using the relu or \"rectified linear\" activation function for our neurons. Instead of writing all of the code for these things, we can just import them, since these are things everyone will be needing in their deep learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module): # few lines of code for neural network that can pass data with an output \n",
    "\n",
    "    def __init__(self): # initializes the self, super, and layers \n",
    "        super().__init__() # sub class inherits the attributes and methods of the nn.module \n",
    "        self.layer1 = nn.Linear(28*28, 64) # first layer takes in 28 x 28 images with outputs 64 connections \n",
    "        self.layer2 = nn.Linear(64, 64) # second layer takes the output from the previous layer\n",
    "        self.layer3 = nn.Linear(64,64) # third layer is meant to repeat \n",
    "        self.layer4 = nn.Linear(64, 10) # 10 neurons for 10 classes  \n",
    "\n",
    "    def forward(self, x): # needs to be called forward just inputs the inputs to feed forward through activation function\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x)) \n",
    "        x = self.layer4(x) \n",
    "        return F.log_softmax(x, dim=1) \n",
    "\n",
    "net = Net() # calls the class Net's function operations \n",
    "print(net) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5371, -2.3832, -2.1664, -2.3416, -2.3503, -2.3378, -2.2496, -2.1220,\n",
       "         -2.2647, -2.3341]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random testing of net () \n",
    "\n",
    "X = torch.randn((28,28)) # random digits \n",
    "X = X.view(-1,28*28) # -1 suggests any size \n",
    "output = net(X) \n",
    "output # tensor with a tensor of our 10 possible classes, sort of pointless without backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing b\n"
     ]
    }
   ],
   "source": [
    "# demo of super and classes: \n",
    "\n",
    "class a:\n",
    "    '''Will be a parent class'''\n",
    "    def __init__(self):\n",
    "        print(\"initializing a\")\n",
    "\n",
    "class b(a):\n",
    "    '''Inherits from a, but does not run a's init method '''\n",
    "    def __init__(self):\n",
    "        print(\"initializing b\")\n",
    "\n",
    "class c(a):\n",
    "    '''Inhereits from a, but DOES run a's init method'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"initializing c\")\n",
    "\n",
    "b_ob = b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing a\n",
      "initializing c\n"
     ]
    }
   ],
   "source": [
    "c_ob = c()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
